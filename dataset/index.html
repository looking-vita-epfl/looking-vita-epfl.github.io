<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
	<link rel="shortcut icon" type="image/x-icon" href="triangle.ico">
	<title>LOOK | In-the-wild eye contact detection benchmark</title>
	<meta name="keywords" content="eye contact, autonomous driving, machine-learning, multitask, computer vision, vision, EPFL, Swiss Federal Institute of Technology, Lausanne">
	<meta name="description" content="Do pedestrians pay attention? In-the-wild eye contact detection - EPFL, Polytech Sorbonne.">
	<!--meta name="google-site-verification" content="tBb4aimY-zxYS_s_z86g6faxMX2G_I_Wp_kzbF3U_RM" />--

	<!-- Google font -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CVarela+Round" rel="stylesheet">

	<!-- Bootstrap -->
	<link type="text/css" rel="stylesheet" href="../css/bootstrap.min.css" />
	<link type="text/css" rel="stylesheet" href="../css/background.css" />

	<!-- Owl Carousel -->
	<link type="text/css" rel="stylesheet" href="../css/owl.carousel.css" />
	<link type="text/css" rel="stylesheet" href="../css/owl.theme.default.css" />

	<!-- Magnific Popup -->
	<link type="text/css" rel="stylesheet" href="../css/magnific-popup.css" />

	<!-- Icons -->
	<link rel="stylesheet" href="css/font-awesome.min.css">
	<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

	<!-- Custom stlylesheet -->
	<link type="text/css" rel="stylesheet" href="../css/style.css" />
	<link type="text/css" rel="stylesheet" href="../css/index.css" />

	<link type="text/css" rel="stylesheet" href="../css/cvpr-video.css"/>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>

	<script src="//code.jquery.com/jquery-3.2.1.min.js"></script>

	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" />
	<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script>



	<link rel="shortcut icon" type="image/x-icon" href="triangle.ico">
</script>


</head>

<body>
	<!-- Header -->
	<header id="home">

		<!-- Background Image -->
		<div style=""  id='particles-js'>
		</div>

		<!-- /Background Image -->

		<!-- Nav -->
		<nav id="nav" class="navbar nav-transparent">
			<div class="container">

				<div class="navbar-header">
					<!-- Logo -->
					<!-- /Logo -->

					<!-- Collapse nav button -->
					<div class="nav-collapse">
						<span></span>
					</div>
					<!-- /Collapse nav button -->
				</div>

				<!--  Main navigation  -->
				<ul class="main-nav nav navbar-nav navbar-right">
					<li><a href="https://looking-vita-epfl.github.io/">Home</a></li>
					<li><a href="https://looking-vita-epfl.github.io/dataset">Dataset</a></li>
					<li><a href="https://looking-vita-epfl.github.io/#paper">Paper</a></li>
					<li><a href="https://github.com/vita-epfl/looking">Code</a></li>
					<li><a href="https://looking-vita-epfl.github.io/#team">Authors</a></li>
					<li><a href="https://looking-vita-epfl.github.io/#licenses">Licenses</a></li>
					<!-- <li class="has-dropdown"><a href="#blog">Blog</a> -->
					<!-- <ul class="dropdown"> -->
					<!-- <li><a href="blog-single.html">blog post</a></li> -->
					<!-- </ul> -->
					<!-- </li> -->
					<!-- <li><a href="#contact">Contact</a></li> -->
				</ul>
				<!-- /Main navigation -->

			</div>
		</nav>
		<!-- /Nav -->
		<!-- /Nav -->

		<!-- home wrapper -->
		<div class="home-wrapper">
			<div class="container">
				<div class="row">

					<!-- home content -->
					<div class="col-md-10 col-md-offset-1">
						<div class="home-content">
							<h2 class="white-text">Do pedestrians pay attention? Eye contact detection in-the-wild</h2><br>
							<h4 class="white-text">Dataset creation page</h4>
							<!--h2 class="white-text">(<u>X-TAC</u>)</h2-->
							<!--<h3 class="white-text">(Under Development. Please check after March 12 for live demo & final content)</h3>	-->




							<a class="white-text2" href="https://younsbelkada.github.io/">Belkada Younes*</a><span style="opacity:0.7">,</span>
							<a class="white-text2" href="https://scholar.google.com/citations?user=f-4YHeMAAAAJ">Bertoni Lorenzo*</a><span style="opacity:0.7">,</span>
							<a class="white-text2" href="https://people.epfl.ch/romain.caristan">Caristan Romain</a><span style="opacity:0.7">,</span>
							<a class="white-text2" href="https://people.epfl.ch/taylor.mordan">Mordan Taylor</a><span style="opacity:0.7">,</span>
							<br>
							<a class="white-text2" href="https://scholar.google.com/citations?user=UIhXQ64AAAAJ&hl=fr">Alexandre Alahi</a><span style="opacity:0.7">,</span>
							<br>
							<br>



							<br>
							<button id="learnmore" class="outline-btn roundedbutton"> <a href="#start">Get started</a></button>						</div>
					</div>
					<!-- /home content -->

				</div>
			</div>
		</div>
		<!-- /home wrapper -->

	</header>
	<!-- /Header -->


	<!-- Teaser -->




	<div>
		<br>

	</div>
		
	<div id="start" class="container">

        <!-- Row -->
        <div class="row">

            <!-- Section header -->
            <div class="section-header text-center">
                <h2 class="title">Creating LOOK Dataset</h2>
            </div>

			<div class="section-header text-center">
                <h3 class="title">Automatic Download</h3>
            </div>
			<div class="abstract-body col-md-12">
				<h3 id="first">1. Downloading the annotation</h3>
				<p>Download the LOOK annotation file <a href="https://drive.google.com/file/d/1SmT-D5E3_CCbAj7BUtvb5GOF2CNsNLhD/view?usp=sharing">here</a>.</p>
                <h3 id="download-the-dataset">2. Download the LOOK images</h3>
				</p>
				The <strong>LOOK</strong> dataset is made of images from 3 different existing datasets where we annotated the pedestrians as looking (1) or not (0) at the camera.
				Download directly the LOOK dataset from <a href="https://zenodo.org/record/5830902">here</a>
				</p>


			<div class="section-header text-center">
                <h3 class="title">Manual Download</h3>
            </div>

            <!-- /Section header -->
            <div class="abstract-body col-md-12">
				<h3 id="first">1. Preparation</h3>
				<p>Start by creating a <code>LOOK</code> folder.</p>
				<p>Download the LOOK annotation file <a href="https://drive.google.com/file/d/1SmT-D5E3_CCbAj7BUtvb5GOF2CNsNLhD/view?usp=sharing">here</a> and copy it inside the <code>LOOK</code> folder.</p>
                <h3 id="download-the-dataset">2. Download the images</h3>
				</p>
				The <strong>LOOK</strong> dataset is made of images from 3 different existing datasets where we annotated the pedestrians as looking (1) or not (0) at the camera.
				</p>
				
	

                <h3 id="nuscenes">Nuscenes</h3>
				Nuscenes dataset contains mainly high resolution images taken from different places (US, Asia). Some scenarios are very crowded and common for self-driving cars.</p>
				<li><strong>nuScenes</strong> contains 1600x900 .jpg images.</li>
                        <div class="figure row">
                            <img width="50%" src="../img/nuscenes.png" alt="" class="center">
                            <p class='col-sm-10 col-sm-offset-1' style='text-align: justify;'>

                            </p>
                        </div>
                Go to <a href="https://www.nuscenes.org/download">Nuscenes official website</a> and download the <code>samples</code> and the <code>CAM_BACK_LEFT</code> <code>sweeps</code> from the server closest to your location (<code>US</code> or <code>ASIA</code>).
                Create a <code>Nuscenes</code>folder inside the <code>LOOK</code> folder and extract the downloaded files there.</li>
				</p>
				<h3 id="jrdb">JRDB</h3>
				JRDB is a dataset collected from a social mobile manipulator JackRabbot from Stanford University. The goal of JRDB is to provide a new source of data and a test-bench for research in the areas of autonomous robot navigation and all perceptual tasks related to social robotics in human environments.</p>
				<li><strong>JRDB</strong> contains 752x480 .jpg images.</li>
				<div class="figure row">
					<img width="50%" src="../img/JRDB_1.jpg" alt="" class="center">
					<p class='col-sm-10 col-sm-offset-1' style='text-align: justify;'>

					</p>
				</div>
                <p>Create a folder <code>JRDB</code> inside <code>LOOK</code>.
                Download the JRDB train dataset from <a href="https://download.cs.stanford.edu/downloads/jrdb/jrdb_train.zip">here</a> and extract it in the <code>JRDB</code> folder you just created.</p>
                <h3 id="KITTI">KITTI</h3>
				KITTI datset is captured by driving around the mid-size city of Karlsruhe, in rural areas and on highways. It is a common dataset and bechmark used for various tasks (3D object detection, depth estimation, ..)</p>
				<li><strong>KITTI</strong> contains .png images at different resolutions.</li>
                        <div class="figure row">
                            <img width="50%" src="../img/kitti.png" alt="" class="center">
                            <p class='col-sm-10 col-sm-offset-1' style='text-align: justify;'>

                            </p>
                        </div>
                <p>Inside the <code>LOOK</code>folder, create a <code>KITTI</code> folder.</p>

                <h5 id="training-data">Training data</h5>
                <p>Create an account at the KITTI Benchmark website and move to the <a href="http://www.cvlibs.net/datasets/kitti/">raw data</a> section. Download the <code>2011_09_29_drive_0071 [synced+rectified data]</code> folder.</p>
                <p>Unzip the folder and copy the <code>2011_09_29/2011_09_29_drive_0071_sync/image_03/data</code> folder into the <code>KITTI</code> folder you created. Rename it <code>train</code>.</p>
                <h5 id="testing-data">Testing data</h5>
                <p>Move to the 2d Object detection benchmark <a href="http://www.cvlibs.net/datasets/KITTI/eval_object.php?obj_benchmark">here</a>. Download the first folder called <code>Download left color images of object data set (12 GB)</code></a>. Unzip the folder and copy the folder <code>data_object_image_2/training/image_2</code> into the <code>KITTI</code> folder. Rename it <code>test</code>.</p>

                <br>
                <br>

		<h3 id="STIP">STIP</h3>
		Follow the instructions from the <a href="https://stip.stanford.edu/dataset.html">STIP official website</a> to get the STIP images.
		<br>
		    
		To get the STIP annotations download everything from <a href="https://drive.google.com/file/d/1r0R-OAlMLoGs-kOJkOnIWUWyZgV5xxWg/view?usp=sharing">this link</a>

		<br>
                <br>

                <h3 id="filter-the-dataset">3. Filter the dataset</h3>
                <p> To keep only the files with a <strong>LOOK</strong> annotation, you need to filter the datasets you previously downloaded. To do so, you can download the <a href="https://drive.google.com/file/d/1TuAFlKynvhKf3m5Agz-fAWL7X82g3k4j/view?usp=sharing">extract_look_dataset.py</a> python script.
                    This script allows you to extract only the annotated images and reorganize them in order the respect the paths of the LOOK dataset.
                    To run this script you need to use python 3 running and the pandas library. <br>
                    To be able to run the script you need to input the name of the dataset(s) you want to filter and the path to the annotation file.
                    To run the script for all the datasets, you can navigate inside the <code>LOOK</code> folder in a terminal window and run the command <br>
                    <code>python extract_look_dataset.py -d Nuscenes,KITTI,JRDB -a LOOK_annotations.csv</code>.
                </p>
                <br>
                <br>

                
            </div>
            <!-- /Row -->

        </div>
        <!-- /Tryit -->

    </div>
